{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "66e855e6-90e2-49b7-80fb-d97865ca7389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d444898a-f789-4647-8817-e068bc9eafe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a rdd (sc = SparkContext)\n",
    "rdd = sc.textFile(\"dbfs:/databricks-datasets/SPARK_README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bd497ae3-9ff2-4e27-91a8-881a9df458bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: ['# Apache Spark',\n '',\n 'Spark is a fast and general cluster computing system for Big Data. It provides',\n 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n 'supports general computation graphs for data analysis. It also supports a',\n 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n 'MLlib for machine learning, GraphX for graph processing,',\n 'and Spark Streaming for stream processing.',\n '',\n '<http://spark.apache.org/>',\n '',\n '',\n '## Online Documentation',\n '',\n 'You can find the latest Spark documentation, including a programming',\n 'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n 'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n 'This README file only contains basic setup instructions.',\n '',\n '## Building Spark']"
     ]
    }
   ],
   "source": [
    "# Read 20 lines \n",
    "rdd.take(20)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47f7e133-2a0c-4b14-a746-04f3a9e38377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\nApache\nSpark\n\nSpark\nis\na\nfast\nand\ngeneral\ncluster\ncomputing\nsystem\nfor\nBig\nData.\nIt\nprovides\nhigh-level\nAPIs\nin\nScala,\nJava,\nPython,\nand\nR,\nand\nan\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nIt\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nSpark\nSQL\nfor\nSQL\nand\nDataFrames,\nMLlib\nfor\nmachine\nlearning,\nGraphX\nfor\ngraph\nprocessing,\nand\nSpark\nStreaming\nfor\nstream\nprocessing.\n\n<http://spark.apache.org/>\n\n\n##\nOnline\nDocumentation\n\nYou\ncan\nfind\nthe\nlatest\nSpark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\nthe\n[project\nweb\npage](http://spark.apache.org/documentation.html)\nand\n[project\nwiki](https://cwiki.apache.org/confluence/display/SPARK).\nThis\nREADME\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nBuilding\nSpark\n\nSpark\nis\nbuilt\nusing\n[Apache\nMaven](http://maven.apache.org/).\nTo\nbuild\nSpark\nand\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-DskipTests\nclean\npackage\n\n(You\ndo\nnot\nneed\nto\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nMore\ndetailed\ndocumentation\nis\navailable\nfrom\nthe\nproject\nsite,\nat\n[\"Building\nSpark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\nInteractive\nScala\nShell\n\nThe\neasiest\nway\nto\nstart\nusing\nSpark\nis\nthrough\nthe\nScala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\nTry\nthe\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala>\nsc.parallelize(1\nto\n1000).count()\n\n##\nInteractive\nPython\nShell\n\nAlternatively,\nif\nyou\nprefer\nPython,\nyou\ncan\nuse\nthe\nPython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nAnd\nrun\nthe\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n>>>\nsc.parallelize(range(1000)).count()\n\n##\nExample\nPrograms\n\nSpark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nin\nthe\n`examples`\ndirectory.\nTo\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nFor\nexample:\n\n\n\n\n\n./bin/run-example\nSparkPi\n\nwill\nrun\nthe\nPi\nexample\nlocally.\n\nYou\ncan\nset\nthe\nMASTER\nenvironment\nvariable\nwhen\nrunning\nexamples\nto\nsubmit\nexamples\nto\na\ncluster.\nThis\ncan\nbe\na\nmesos://\nor\nspark://\nURL,\n\"yarn\"\nto\nrun\non\nYARN,\nand\n\"local\"\nto\nrun\nlocally\nwith\none\nthread,\nor\n\"local[N]\"\nto\nrun\nlocally\nwith\nN\nthreads.\nYou\ncan\nalso\nuse\nan\nabbreviated\nclass\nname\nif\nthe\nclass\nis\nin\nthe\n`examples`\npackage.\nFor\ninstance:\n\n\n\n\n\nMASTER=spark://host:7077\n./bin/run-example\nSparkPi\n\nMany\nof\nthe\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nRunning\nTests\n\nTesting\nfirst\nrequires\n[building\nSpark](#building-spark).\nOnce\nSpark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nPlease\nsee\nthe\nguidance\non\nhow\nto\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\n\n##\nA\nNote\nAbout\nHadoop\nVersions\n\nSpark\nuses\nthe\nHadoop\ncore\nlibrary\nto\ntalk\nto\nHDFS\nand\nother\nHadoop-supported\nstorage\nsystems.\nBecause\nthe\nprotocols\nhave\nchanged\nin\ndifferent\nversions\nof\nHadoop,\nyou\nmust\nbuild\nSpark\nagainst\nthe\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nPlease\nrefer\nto\nthe\nbuild\ndocumentation\nat\n[\"Specifying\nthe\nHadoop\nVersion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nHadoop,\nincluding\nbuilding\nfor\nparticular\nHive\nand\nHive\nThriftserver\ndistributions.\n\n##\nConfiguration\n\nPlease\nrefer\nto\nthe\n[Configuration\nGuide](http://spark.apache.org/docs/latest/configuration.html)\nin\nthe\nonline\ndocumentation\nfor\nan\noverview\non\nhow\nto\nconfigure\nSpark.\n"
     ]
    }
   ],
   "source": [
    "words = rdd.flatMap(lambda lines: lines.split(\" \"))\n",
    "\n",
    "for w in words.collect():\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce997dfc-15bc-4156-b088-fb5ecd75fbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#counting the occurence of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b4bb4225-7e39-4aae-acb3-ca0a40a5de27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 1), ('Apache', 1), ('Spark', 13), ('', 67), ('is', 6), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 5), ('Scala,', 1), ('Java,', 1), ('an', 3), ('optimized', 1), ('engine', 1), ('supports', 2), ('computation', 1), ('analysis.', 1), ('set', 2), ('of', 5), ('tools', 1), ('SQL', 2), ('MLlib', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('processing,', 1), ('Documentation', 1), ('latest', 1), ('programming', 1), ('guide,', 1), ('[project', 2), ('README', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('Building', 1), ('using', 2), ('[Apache', 1), ('run:', 1), ('do', 2), ('this', 1), ('downloaded', 1), ('documentation', 3), ('project', 1), ('site,', 1), ('at', 2), ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('Interactive', 2), ('Shell', 2), ('The', 1), ('way', 1), ('start', 1), ('Try', 1), ('following', 2), ('1000:', 2), ('scala>', 1), ('1000).count()', 1), ('Python', 2), ('Alternatively,', 1), ('use', 3), ('And', 1), ('run', 7), ('Example', 1), ('several', 1), ('programs', 2), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('./bin/run-example', 2), ('SparkPi', 2), ('variable', 1), ('when', 1), ('examples', 2), ('spark://', 1), ('URL,', 1), ('YARN,', 1), ('\"local\"', 1), ('locally', 2), ('N', 1), ('abbreviated', 1), ('class', 2), ('name', 1), ('package.', 1), ('instance:', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('Testing', 1), ('Spark](#building-spark).', 1), ('Once', 1), ('built,', 1), ('tests', 2), ('using:', 1), ('./dev/run-tests', 1), ('Please', 3), ('guidance', 2), ('module,', 1), ('individual', 1), ('Note', 1), ('About', 1), ('uses', 1), ('library', 1), ('HDFS', 1), ('other', 1), ('Hadoop-supported', 1), ('storage', 1), ('systems.', 1), ('Because', 1), ('have', 1), ('changed', 1), ('different', 1), ('versions', 1), ('Hadoop,', 2), ('must', 1), ('against', 1), ('version', 1), ('refer', 2), ('particular', 2), ('distribution', 1), ('Hive', 2), ('Thriftserver', 1), ('distributions.', 1), ('[Configuration', 1), ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('online', 1), ('overview', 1), ('configure', 1), ('Spark.', 1), ('a', 8), ('fast', 1), ('and', 10), ('general', 2), ('cluster', 2), ('computing', 1), ('system', 1), ('for', 11), ('Big', 1), ('Data.', 1), ('Python,', 2), ('R,', 1), ('that', 2), ('graphs', 1), ('data', 1), ('also', 4), ('rich', 1), ('higher-level', 1), ('including', 3), ('DataFrames,', 1), ('Streaming', 1), ('stream', 1), ('processing.', 1), ('<http://spark.apache.org/>', 1), ('##', 8), ('Online', 1), ('You', 3), ('can', 6), ('find', 1), ('the', 21), ('documentation,', 1), ('on', 5), ('web', 1), ('page](http://spark.apache.org/documentation.html)', 1), ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), ('This', 2), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('Maven](http://maven.apache.org/).', 1), ('To', 2), ('build', 3), ('its', 1), ('example', 3), ('programs,', 1), ('build/mvn', 1), ('-DskipTests', 1), ('clean', 1), ('package', 1), ('(You', 1), ('not', 1), ('need', 1), ('to', 14), ('if', 4), ('you', 4), ('pre-built', 1), ('package.)', 1), ('More', 1), ('detailed', 2), ('available', 1), ('from', 1), ('[\"Building', 1), ('Scala', 2), ('easiest', 1), ('through', 1), ('shell:', 2), ('./bin/spark-shell', 1), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('sc.parallelize(1', 1), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('sc.parallelize(range(1000)).count()', 1), ('Programs', 1), ('comes', 1), ('with', 3), ('sample', 1), ('`examples`', 2), ('directory.', 1), ('one', 2), ('<class>', 1), ('For', 2), ('will', 1), ('Pi', 1), ('locally.', 1), ('MASTER', 1), ('environment', 1), ('running', 1), ('submit', 1), ('cluster.', 1), ('be', 2), ('mesos://', 1), ('or', 3), ('\"yarn\"', 1), ('thread,', 1), ('\"local[N]\"', 1), ('threads.', 1), ('MASTER=spark://host:7077', 1), ('Many', 1), ('given.', 1), ('Running', 1), ('Tests', 1), ('first', 1), ('requires', 1), ('[building', 1), ('see', 1), ('how', 2), ('[run', 1), ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), ('A', 1), ('Hadoop', 3), ('Versions', 1), ('core', 1), ('talk', 1), ('protocols', 1), ('same', 1), ('your', 1), ('runs.', 1), ('[\"Specifying', 1), ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('building', 2), ('Configuration', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Split lines into words using flatMap\n",
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Convert words into (word, 1) pairs\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key to count occurrences\n",
    "word_count_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and display the result\n",
    "print(word_count_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8dea54c-e50a-4a52-b521-aedaa2cacaaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#changing all capital letters to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8b56055f-8ae6-4840-8642-395c85e1d376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# apache spark', '', 'spark is a fast and general cluster computing system for big data. it provides', 'high-level apis in scala, java, python, and r, and an optimized engine that', 'supports general computation graphs for data analysis. it also supports a', 'rich set of higher-level tools including spark sql for sql and dataframes,', 'mllib for machine learning, graphx for graph processing,', 'and spark streaming for stream processing.', '', '<http://spark.apache.org/>', '', '', '## online documentation', '', 'you can find the latest spark documentation, including a programming', 'guide, on the [project web page](http://spark.apache.org/documentation.html)', 'and [project wiki](https://cwiki.apache.org/confluence/display/spark).', 'this readme file only contains basic setup instructions.', '', '## building spark', '', 'spark is built using [apache maven](http://maven.apache.org/).', 'to build spark and its example programs, run:', '', '    build/mvn -dskiptests clean package', '', '(you do not need to do this if you downloaded a pre-built package.)', 'more detailed documentation is available from the project site, at', '[\"building spark\"](http://spark.apache.org/docs/latest/building-spark.html).', '', '## interactive scala shell', '', 'the easiest way to start using spark is through the scala shell:', '', '    ./bin/spark-shell', '', 'try the following command, which should return 1000:', '', '    scala> sc.parallelize(1 to 1000).count()', '', '## interactive python shell', '', 'alternatively, if you prefer python, you can use the python shell:', '', '    ./bin/pyspark', '', 'and run the following command, which should also return 1000:', '', '    >>> sc.parallelize(range(1000)).count()', '', '## example programs', '', 'spark also comes with several sample programs in the `examples` directory.', 'to run one of them, use `./bin/run-example <class> [params]`. for example:', '', '    ./bin/run-example sparkpi', '', 'will run the pi example locally.', '', 'you can set the master environment variable when running examples to submit', 'examples to a cluster. this can be a mesos:// or spark:// url,', '\"yarn\" to run on yarn, and \"local\" to run', 'locally with one thread, or \"local[n]\" to run locally with n threads. you', 'can also use an abbreviated class name if the class is in the `examples`', 'package. for instance:', '', '    master=spark://host:7077 ./bin/run-example sparkpi', '', 'many of the example programs print usage help if no params are given.', '', '## running tests', '', 'testing first requires [building spark](#building-spark). once spark is built, tests', 'can be run using:', '', '    ./dev/run-tests', '', 'please see the guidance on how to', '[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).', '', '## a note about hadoop versions', '', 'spark uses the hadoop core library to talk to hdfs and other hadoop-supported', 'storage systems. because the protocols have changed in different versions of', 'hadoop, you must build spark against the same version that your cluster runs.', '', 'please refer to the build documentation at', '[\"specifying the hadoop version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 'for detailed guidance on building for a particular distribution of hadoop, including', 'building for particular hive and hive thriftserver distributions.', '', '## configuration', '', 'please refer to the [configuration guide](http://spark.apache.org/docs/latest/configuration.html)', 'in the online documentation for an overview on how to configure spark.']\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lowercase using map()\n",
    "lower_rdd = rdd.map(lambda line: line.lower())\n",
    "\n",
    "# Collect and print results\n",
    "print(lower_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1845675a-beb1-452e-9fd4-7c4f0dc086df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Eliminate stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e31b5f76-c2a4-455b-8fec-c85e9e89091b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'apache', 'spark', '', 'spark', 'is', 'a', 'fast', 'general', 'cluster', 'computing', 'system', 'for', 'big', 'data.', 'it', 'provides', 'high-level', 'apis', 'scala,', 'java,', 'python,', 'r,', 'optimized', 'engine', 'that', 'supports', 'general', 'computation', 'graphs', 'for', 'data', 'analysis.', 'it', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'spark', 'sql', 'for', 'sql', 'dataframes,', 'mllib', 'for', 'machine', 'learning,', 'graphx', 'for', 'graph', 'processing,', 'spark', 'streaming', 'for', 'stream', 'processing.', '', '<http://spark.apache.org/>', '', '', '##', 'online', 'documentation', '', 'you', 'can', 'find', 'latest', 'spark', 'documentation,', 'including', 'a', 'programming', 'guide,', 'on', '[project', 'web', 'page](http://spark.apache.org/documentation.html)', '[project', 'wiki](https://cwiki.apache.org/confluence/display/spark).', 'this', 'readme', 'file', 'only', 'contains', 'basic', 'setup', 'instructions.', '', '##', 'building', 'spark', '', 'spark', 'is', 'built', 'using', '[apache', 'maven](http://maven.apache.org/).', 'build', 'spark', 'its', 'example', 'programs,', 'run:', '', '', '', '', '', 'build/mvn', '-dskiptests', 'clean', 'package', '', '(you', 'do', 'not', 'need', 'do', 'this', 'if', 'you', 'downloaded', 'a', 'pre-built', 'package.)', 'more', 'detailed', 'documentation', 'is', 'available', 'from', 'project', 'site,', '[\"building', 'spark\"](http://spark.apache.org/docs/latest/building-spark.html).', '', '##', 'interactive', 'scala', 'shell', '', 'easiest', 'way', 'start', 'using', 'spark', 'is', 'through', 'scala', 'shell:', '', '', '', '', '', './bin/spark-shell', '', 'try', 'following', 'command,', 'which', 'should', 'return', '1000:', '', '', '', '', '', 'scala>', 'sc.parallelize(1', '1000).count()', '', '##', 'interactive', 'python', 'shell', '', 'alternatively,', 'if', 'you', 'prefer', 'python,', 'you', 'can', 'use', 'python', 'shell:', '', '', '', '', '', './bin/pyspark', '', 'run', 'following', 'command,', 'which', 'should', 'also', 'return', '1000:', '', '', '', '', '', '>>>', 'sc.parallelize(range(1000)).count()', '', '##', 'example', 'programs', '', 'spark', 'also', 'comes', 'with', 'several', 'sample', 'programs', '`examples`', 'directory.', 'run', 'one', 'of', 'them,', 'use', '`./bin/run-example', '<class>', '[params]`.', 'for', 'example:', '', '', '', '', '', './bin/run-example', 'sparkpi', '', 'will', 'run', 'pi', 'example', 'locally.', '', 'you', 'can', 'set', 'master', 'environment', 'variable', 'when', 'running', 'examples', 'submit', 'examples', 'a', 'cluster.', 'this', 'can', 'be', 'a', 'mesos://', 'or', 'spark://', 'url,', '\"yarn\"', 'run', 'on', 'yarn,', '\"local\"', 'run', 'locally', 'with', 'one', 'thread,', 'or', '\"local[n]\"', 'run', 'locally', 'with', 'n', 'threads.', 'you', 'can', 'also', 'use', 'abbreviated', 'class', 'name', 'if', 'class', 'is', '`examples`', 'package.', 'for', 'instance:', '', '', '', '', '', 'master=spark://host:7077', './bin/run-example', 'sparkpi', '', 'many', 'of', 'example', 'programs', 'print', 'usage', 'help', 'if', 'no', 'params', 'are', 'given.', '', '##', 'running', 'tests', '', 'testing', 'first', 'requires', '[building', 'spark](#building-spark).', 'once', 'spark', 'is', 'built,', 'tests', 'can', 'be', 'run', 'using:', '', '', '', '', '', './dev/run-tests', '', 'please', 'see', 'guidance', 'on', 'how', '[run', 'tests', 'for', 'a', 'module,', 'or', 'individual', 'tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).', '', '##', 'a', 'note', 'about', 'hadoop', 'versions', '', 'spark', 'uses', 'hadoop', 'core', 'library', 'talk', 'hdfs', 'other', 'hadoop-supported', 'storage', 'systems.', 'because', 'protocols', 'have', 'changed', 'different', 'versions', 'of', 'hadoop,', 'you', 'must', 'build', 'spark', 'against', 'same', 'version', 'that', 'your', 'cluster', 'runs.', '', 'please', 'refer', 'build', 'documentation', '[\"specifying', 'hadoop', 'version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 'for', 'detailed', 'guidance', 'on', 'building', 'for', 'a', 'particular', 'distribution', 'of', 'hadoop,', 'including', 'building', 'for', 'particular', 'hive', 'hive', 'thriftserver', 'distributions.', '', '##', 'configuration', '', 'please', 'refer', '[configuration', 'guide](http://spark.apache.org/docs/latest/configuration.html)', 'online', 'documentation', 'for', 'overview', 'on', 'how', 'configure', 'spark.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['and', 'to', 'in', 'at', 'the', 'an']\n",
    "#  Convert text to words using flatMap()\n",
    "words_rdd = rdd.flatMap(lambda line: line.lower().split(\" \"))\n",
    "\n",
    "# Filter out stop words\n",
    "filtered_rdd = words_rdd.filter(lambda word: word not in stop_words)\n",
    "\n",
    "# Collect and print the result\n",
    "print(filtered_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "844402b1-10e3-4d65-907f-9f04e7505130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#sorting list in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7553e51b-39ea-4b5e-891f-b1687551cd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '    ./bin/pyspark', '    ./bin/run-example sparkpi', '    ./bin/spark-shell', '    ./dev/run-tests', '    >>> sc.parallelize(range(1000)).count()', '    build/mvn -dskiptests clean package', '    master=spark://host:7077 ./bin/run-example sparkpi', '    scala> sc.parallelize(1 to 1000).count()', '\"yarn\" to run on yarn, and \"local\" to run', '# apache spark', '## a note about hadoop versions', '## building spark', '## configuration', '## example programs', '## interactive python shell', '## interactive scala shell', '## online documentation', '## running tests', '(you do not need to do this if you downloaded a pre-built package.)', '<http://spark.apache.org/>', '[\"building spark\"](http://spark.apache.org/docs/latest/building-spark.html).', '[\"specifying the hadoop version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', '[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).', 'alternatively, if you prefer python, you can use the python shell:', 'and [project wiki](https://cwiki.apache.org/confluence/display/spark).', 'and run the following command, which should also return 1000:', 'and spark streaming for stream processing.', 'building for particular hive and hive thriftserver distributions.', 'can also use an abbreviated class name if the class is in the `examples`', 'can be run using:', 'examples to a cluster. this can be a mesos:// or spark:// url,', 'for detailed guidance on building for a particular distribution of hadoop, including', 'guide, on the [project web page](http://spark.apache.org/documentation.html)', 'hadoop, you must build spark against the same version that your cluster runs.', 'high-level apis in scala, java, python, and r, and an optimized engine that', 'in the online documentation for an overview on how to configure spark.', 'locally with one thread, or \"local[n]\" to run locally with n threads. you', 'many of the example programs print usage help if no params are given.', 'mllib for machine learning, graphx for graph processing,', 'more detailed documentation is available from the project site, at', 'package. for instance:', 'please refer to the [configuration guide](http://spark.apache.org/docs/latest/configuration.html)', 'please refer to the build documentation at', 'please see the guidance on how to', 'rich set of higher-level tools including spark sql for sql and dataframes,', 'spark also comes with several sample programs in the `examples` directory.', 'spark is a fast and general cluster computing system for big data. it provides', 'spark is built using [apache maven](http://maven.apache.org/).', 'spark uses the hadoop core library to talk to hdfs and other hadoop-supported', 'storage systems. because the protocols have changed in different versions of', 'supports general computation graphs for data analysis. it also supports a', 'testing first requires [building spark](#building-spark). once spark is built, tests', 'the easiest way to start using spark is through the scala shell:', 'this readme file only contains basic setup instructions.', 'to build spark and its example programs, run:', 'to run one of them, use `./bin/run-example <class> [params]`. for example:', 'try the following command, which should return 1000:', 'will run the pi example locally.', 'you can find the latest spark documentation, including a programming', 'you can set the master environment variable when running examples to submit']\n"
     ]
    }
   ],
   "source": [
    "# Convert all words to lowercase and sort alphabetically\n",
    "sorted_rdd = rdd.map(lambda word: word.lower()).sortBy(lambda word: word)\n",
    "\n",
    "# Collect and print results\n",
    "print(sorted_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca2247aa-2426-4019-9ce8-8ab1c16144b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#sorting list from most to least frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "622bebb4-8dbf-4404-8c96-310753bcccf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 67), ('the', 22), ('to', 16), ('spark', 13), ('for', 13), ('and', 11), ('a', 9), ('##', 8), ('run', 7), ('you', 7), ('is', 6), ('can', 6), ('in', 5), ('of', 5), ('on', 5), ('documentation', 4), ('also', 4), ('example', 4), ('if', 4), ('an', 3), ('this', 3), ('use', 3), ('programs', 3), ('tests', 3), ('hadoop', 3), ('including', 3), ('building', 3), ('build', 3), ('with', 3), ('or', 3), ('please', 3), ('supports', 2), ('set', 2), ('online', 2), ('[project', 2), ('using', 2), ('do', 2), ('at', 2), ('scala', 2), ('shell', 2), ('following', 2), ('1000:', 2), ('python', 2), ('./bin/run-example', 2), ('examples', 2), ('locally', 2), ('class', 2), ('guidance', 2), ('versions', 2), ('hadoop,', 2), ('refer', 2), ('particular', 2), ('hive', 2), ('general', 2), ('cluster', 2), ('it', 2), ('python,', 2), ('that', 2), ('sql', 2), ('detailed', 2), ('interactive', 2), ('shell:', 2), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('`examples`', 2), ('one', 2), ('sparkpi', 2), ('running', 2), ('be', 2), ('how', 2), ('#', 1), ('data.', 1), ('provides', 1), ('high-level', 1), ('java,', 1), ('r,', 1), ('optimized', 1), ('engine', 1), ('computation', 1), ('analysis.', 1), ('tools', 1), ('dataframes,', 1), ('machine', 1), ('learning,', 1), ('graph', 1), ('processing,', 1), ('streaming', 1), ('latest', 1), ('programming', 1), ('guide,', 1), ('wiki](https://cwiki.apache.org/confluence/display/spark).', 1), ('readme', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('maven](http://maven.apache.org/).', 1), ('run:', 1), ('(you', 1), ('downloaded', 1), ('more', 1), ('project', 1), ('site,', 1), ('spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('way', 1), ('start', 1), ('try', 1), ('scala>', 1), ('1000).count()', 1), ('alternatively,', 1), ('several', 1), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('master', 1), ('variable', 1), ('when', 1), ('spark://', 1), ('url,', 1), ('yarn,', 1), ('\"local\"', 1), ('\"local[n]\"', 1), ('abbreviated', 1), ('name', 1), ('package.', 1), ('instance:', 1), ('master=spark://host:7077', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('once', 1), ('built,', 1), ('using:', 1), ('./dev/run-tests', 1), ('module,', 1), ('individual', 1), ('tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).', 1), ('uses', 1), ('library', 1), ('hdfs', 1), ('other', 1), ('storage', 1), ('systems.', 1), ('have', 1), ('changed', 1), ('different', 1), ('must', 1), ('against', 1), ('version', 1), ('[\"specifying', 1), ('version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('distribution', 1), ('distributions.', 1), ('configuration', 1), ('guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('overview', 1), ('configure', 1), ('spark.', 1), ('apache', 1), ('fast', 1), ('computing', 1), ('system', 1), ('big', 1), ('apis', 1), ('scala,', 1), ('graphs', 1), ('data', 1), ('rich', 1), ('higher-level', 1), ('mllib', 1), ('graphx', 1), ('stream', 1), ('processing.', 1), ('<http://spark.apache.org/>', 1), ('find', 1), ('documentation,', 1), ('web', 1), ('page](http://spark.apache.org/documentation.html)', 1), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('[apache', 1), ('its', 1), ('programs,', 1), ('build/mvn', 1), ('-dskiptests', 1), ('clean', 1), ('package', 1), ('not', 1), ('need', 1), ('pre-built', 1), ('package.)', 1), ('available', 1), ('from', 1), ('[\"building', 1), ('easiest', 1), ('through', 1), ('./bin/spark-shell', 1), ('sc.parallelize(1', 1), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('sc.parallelize(range(1000)).count()', 1), ('comes', 1), ('sample', 1), ('directory.', 1), ('<class>', 1), ('will', 1), ('pi', 1), ('locally.', 1), ('environment', 1), ('submit', 1), ('cluster.', 1), ('mesos://', 1), ('\"yarn\"', 1), ('thread,', 1), ('n', 1), ('threads.', 1), ('many', 1), ('given.', 1), ('testing', 1), ('first', 1), ('requires', 1), ('[building', 1), ('spark](#building-spark).', 1), ('see', 1), ('[run', 1), ('note', 1), ('about', 1), ('core', 1), ('talk', 1), ('hadoop-supported', 1), ('because', 1), ('protocols', 1), ('same', 1), ('your', 1), ('runs.', 1), ('thriftserver', 1), ('[configuration', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Split lines into words (case-insensitive)\n",
    "words_rdd = rdd.flatMap(lambda line: line.lower().split(\" \"))\n",
    "\n",
    "# Create (word, 1) key-value pairs\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_count_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort by frequency in descending order\n",
    "sorted_rdd = word_count_rdd.sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "# Collect and print results\n",
    "print(sorted_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e46eb436-96d7-4ebc-90f7-783e21bf754e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1571e4e-ba64-4ca6-acd5-a4d2fc81e997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache', 'Spark', 'Spark', 'is', 'a', 'fast', 'and', 'general', 'cluster', 'computing', 'system', 'for', 'Big', 'Data', 'It', 'provides', 'highlevel', 'APIs', 'in', 'Scala', 'Java', 'Python', 'and', 'R', 'and', 'an', 'optimized', 'engine', 'that', 'supports', 'general', 'computation', 'graphs', 'for', 'data', 'analysis', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higherlevel', 'tools', 'including', 'Spark', 'SQL', 'for', 'SQL', 'and', 'DataFrames', 'MLlib', 'for', 'machine', 'learning', 'GraphX', 'for', 'graph', 'processing', 'and', 'Spark', 'Streaming', 'for', 'stream', 'processing', 'httpsparkapacheorg', 'Online', 'Documentation', 'You', 'can', 'find', 'the', 'latest', 'Spark', 'documentation', 'including', 'a', 'programming', 'guide', 'on', 'the', 'project', 'web', 'pagehttpsparkapacheorgdocumentationhtml', 'and', 'project', 'wikihttpscwikiapacheorgconfluencedisplaySPARK', 'This', 'README', 'file', 'only', 'contains', 'basic', 'setup', 'instructions', 'Building', 'Spark', 'Spark', 'is', 'built', 'using', 'Apache', 'Mavenhttpmavenapacheorg', 'To', 'build', 'Spark', 'and', 'its', 'example', 'programs', 'run', 'buildmvn', 'DskipTests', 'clean', 'package', 'You', 'do', 'not', 'need', 'to', 'do', 'this', 'if', 'you', 'downloaded', 'a', 'prebuilt', 'package', 'More', 'detailed', 'documentation', 'is', 'available', 'from', 'the', 'project', 'site', 'at', 'Building', 'Sparkhttpsparkapacheorgdocslatestbuildingsparkhtml', 'Interactive', 'Scala', 'Shell', 'The', 'easiest', 'way', 'to', 'start', 'using', 'Spark', 'is', 'through', 'the', 'Scala', 'shell', 'binsparkshell', 'Try', 'the', 'following', 'command', 'which', 'should', 'return', '1000', 'scala', 'scparallelize1', 'to', '1000count', 'Interactive', 'Python', 'Shell', 'Alternatively', 'if', 'you', 'prefer', 'Python', 'you', 'can', 'use', 'the', 'Python', 'shell', 'binpyspark', 'And', 'run', 'the', 'following', 'command', 'which', 'should', 'also', 'return', '1000', 'scparallelizerange1000count', 'Example', 'Programs', 'Spark', 'also', 'comes', 'with', 'several', 'sample', 'programs', 'in', 'the', 'examples', 'directory', 'To', 'run', 'one', 'of', 'them', 'use', 'binrunexample', 'class', 'params', 'For', 'example', 'binrunexample', 'SparkPi', 'will', 'run', 'the', 'Pi', 'example', 'locally', 'You', 'can', 'set', 'the', 'MASTER', 'environment', 'variable', 'when', 'running', 'examples', 'to', 'submit', 'examples', 'to', 'a', 'cluster', 'This', 'can', 'be', 'a', 'mesos', 'or', 'spark', 'URL', 'yarn', 'to', 'run', 'on', 'YARN', 'and', 'local', 'to', 'run', 'locally', 'with', 'one', 'thread', 'or', 'localN', 'to', 'run', 'locally', 'with', 'N', 'threads', 'You', 'can', 'also', 'use', 'an', 'abbreviated', 'class', 'name', 'if', 'the', 'class', 'is', 'in', 'the', 'examples', 'package', 'For', 'instance', 'MASTERsparkhost7077', 'binrunexample', 'SparkPi', 'Many', 'of', 'the', 'example', 'programs', 'print', 'usage', 'help', 'if', 'no', 'params', 'are', 'given', 'Running', 'Tests', 'Testing', 'first', 'requires', 'building', 'Sparkbuildingspark', 'Once', 'Spark', 'is', 'built', 'tests', 'can', 'be', 'run', 'using', 'devruntests', 'Please', 'see', 'the', 'guidance', 'on', 'how', 'to', 'run', 'tests', 'for', 'a', 'module', 'or', 'individual', 'testshttpscwikiapacheorgconfluencedisplaySPARKUsefulDeveloperTools', 'A', 'Note', 'About', 'Hadoop', 'Versions', 'Spark', 'uses', 'the', 'Hadoop', 'core', 'library', 'to', 'talk', 'to', 'HDFS', 'and', 'other', 'Hadoopsupported', 'storage', 'systems', 'Because', 'the', 'protocols', 'have', 'changed', 'in', 'different', 'versions', 'of', 'Hadoop', 'you', 'must', 'build', 'Spark', 'against', 'the', 'same', 'version', 'that', 'your', 'cluster', 'runs', 'Please', 'refer', 'to', 'the', 'build', 'documentation', 'at', 'Specifying', 'the', 'Hadoop', 'Versionhttpsparkapacheorgdocslatestbuildingsparkhtmlspecifyingthehadoopversion', 'for', 'detailed', 'guidance', 'on', 'building', 'for', 'a', 'particular', 'distribution', 'of', 'Hadoop', 'including', 'building', 'for', 'particular', 'Hive', 'and', 'Hive', 'Thriftserver', 'distributions', 'Configuration', 'Please', 'refer', 'to', 'the', 'Configuration', 'Guidehttpsparkapacheorgdocslatestconfigurationhtml', 'in', 'the', 'online', 'documentation', 'for', 'an', 'overview', 'on', 'how', 'to', 'configure', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Function to remove punctuation using regex\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)  # Removes all non-alphanumeric characters except spaces\n",
    "\n",
    "# Split text into words\n",
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_rdd = words_rdd.map(lambda word: remove_punctuation(word))\n",
    "\n",
    "# Filter out empty words (in case punctuation removal leaves empty strings)\n",
    "filtered_rdd = cleaned_rdd.filter(lambda word: word.strip() != \"\")\n",
    "\n",
    "# Collect and print the result\n",
    "print(filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7163a08-3d05-463d-9001-eaf78b0e1ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)])\n",
    "\n",
    "# Try to undestand what this code does (line by line)\n",
    "agesRDD = (dataRDD\n",
    "  .map(lambda x: (x[0], (x[1], 1)))\n",
    "  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "  .map(lambda x: (x[0], x[1][0]/x[1][1])))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-02-12 11:25:52",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
